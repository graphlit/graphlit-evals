{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZNrWnoNDNGYyeCnVTrbgo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/graphlit/graphlit-evals/blob/main/tonic-validate/Tonic_Validate_Graphlit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade tonic_validate"
      ],
      "metadata": {
        "id": "KLtOBMHf503H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fefizrrh4xGD"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade graphlit-client"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import Optional\n",
        "from tonic_validate import ValidateScorer, Benchmark, BenchmarkItem, LLMResponse, BenchmarkItem, Run\n",
        "from tonic_validate.metrics import AnswerSimilarityMetric\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "nadxflGVLSvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# configure shared, writable folder containing sample data\n",
        "tonic_validate_directory = \"/content/drive/MyDrive/Colab Notebooks/Tonic Validate\""
      ],
      "metadata": {
        "id": "zT4ZgOGB6Vtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize Graphlit"
      ],
      "metadata": {
        "id": "abV1114jL-bR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from graphlit import Graphlit\n",
        "from graphlit_api import input_types, enums, exceptions\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "os.environ['GRAPHLIT_ORGANIZATION_ID'] = userdata.get('GRAPHLIT_ORGANIZATION_ID')\n",
        "os.environ['GRAPHLIT_ENVIRONMENT_ID'] = userdata.get('GRAPHLIT_ENVIRONMENT_ID')\n",
        "os.environ['GRAPHLIT_JWT_SECRET'] = userdata.get('GRAPHLIT_JWT_SECRET')\n",
        "\n",
        "graphlit = Graphlit()"
      ],
      "metadata": {
        "id": "WoMAWD4LLP_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load benchmark questions & answers"
      ],
      "metadata": {
        "id": "zImyrtBNMPNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f\"{tonic_validate_directory}/Sample Data/qa_pairs.json\", \"r\") as f:\n",
        "    qa_pairs = json.load(f)\n",
        "\n",
        "# for testing\n",
        "qa_pairs = qa_pairs\n",
        "\n",
        "benchmark = Benchmark(\n",
        "    questions=[x[\"question\"] for x in qa_pairs],\n",
        "    answers=[x[\"answer\"] for x in qa_pairs]\n",
        ")\n",
        "\n",
        "def run_to_dataframe(run: Run) -> pd.DataFrame:\n",
        "    return pd.DataFrame(\n",
        "        {\n",
        "            \"reference_question\": [x.reference_question for x in run.run_data],\n",
        "            \"reference_answer\": [x.reference_answer for x in run.run_data],\n",
        "            \"llm_answer\": [x.llm_answer for x in run.run_data],\n",
        "            \"llm_context\": [json.dumps(x.llm_context) for x in run.run_data],\n",
        "            \"answer_similarity\": [x.scores[\"answer_similarity\"] for x in run.run_data]\n",
        "        }\n",
        "    )"
      ],
      "metadata": {
        "id": "AcMz7iV741mP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Graphlit helper functions"
      ],
      "metadata": {
        "id": "pgRX57EHMVfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "async def upload_graphlit_essays():\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    directory = f\"{tonic_validate_directory}/Sample Data/paul_graham_essays\"\n",
        "\n",
        "    response = await graphlit.client.create_collection(input_types.CollectionInput(name=\"Paul Graham Essays\"))\n",
        "\n",
        "    collection_id = response.create_collection.id if response.create_collection is not None else None\n",
        "\n",
        "    if collection_id is not None:\n",
        "        file_paths = [os.path.join(directory, filename)\n",
        "                    for filename in os.listdir(directory)\n",
        "                    if os.path.isfile(os.path.join(directory, filename))]\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        tasks = []\n",
        "        for file_path in file_paths:\n",
        "            file_name = os.path.basename(file_path)\n",
        "            content_name, _ = os.path.splitext(file_name)\n",
        "\n",
        "            with open(file_path, \"r\", encoding='utf-8') as file:\n",
        "                file_content = file.read()\n",
        "\n",
        "            task = graphlit.client.ingest_text(content_name, file_content, text_type=enums.TextTypes.PLAIN,\n",
        "                is_synchronous=True, collections=[input_types.EntityReferenceInput(id=collection_id)])\n",
        "            tasks.append(task)\n",
        "\n",
        "        results = await asyncio.gather(*tasks)\n",
        "\n",
        "        duration = time.time() - start_time\n",
        "\n",
        "        current_time = datetime.now()\n",
        "        formatted_time = current_time.strftime(\"%H:%M:%S\")\n",
        "\n",
        "        print(f\"Uploading essays took {duration:.2f} seconds. Finished at {formatted_time} UTC.\")\n",
        "\n",
        "        return collection_id\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "async def create_specification():\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    input = input_types.SpecificationInput(\n",
        "        name=\"Completion\",\n",
        "        type=enums.SpecificationTypes.COMPLETION,\n",
        "        serviceType=enums.ModelServiceTypes.OPEN_AI,\n",
        "        openAI=input_types.OpenAIModelPropertiesInput(model=enums.OpenAIModels.GPT4_TURBO_128K),\n",
        "        searchType=enums.ConversationSearchTypes.VECTOR,\n",
        "        numberSimilar=25,\n",
        "#        promptStrategy=input_types.PromptStrategyInput(\n",
        "#            type=enums.PromptStrategyTypes.REWRITE_QUESTION\n",
        "#        ),\n",
        "        retrievalStrategy=input_types.RetrievalStrategyInput(\n",
        "            type=enums.RetrievalStrategyTypes.CHUNK,\n",
        "            contentLimit=10,\n",
        "        ),\n",
        "        rerankingStrategy=input_types.RerankingStrategyInput(\n",
        "            serviceType=enums.RerankingModelServiceTypes.COHERE\n",
        "        )\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.create_specification(input)\n",
        "\n",
        "        return response.create_specification.id if response.create_specification is not None else None\n",
        "    except exceptions.GraphQLClientError as e:\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "# NOTE: for cleaning up project data\n",
        "async def delete_all_collections():\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    _ = await graphlit.client.delete_all_collections()\n",
        "\n",
        "    print('Deleted all collections.')\n",
        "\n",
        "async def delete_all_contents():\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    _ = await graphlit.client.delete_all_contents()\n",
        "\n",
        "    print('Deleted all contents.')"
      ],
      "metadata": {
        "id": "mtwjJsvVOVCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Graphlit RAG function"
      ],
      "metadata": {
        "id": "sw9QCQ_qMh5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def get_graphlit_rag_response(benchmarkItem: BenchmarkItem, specification_id: str, collection_id: str):\n",
        "    if graphlit.client is None:\n",
        "        return;\n",
        "\n",
        "    prompt = benchmarkItem.question\n",
        "\n",
        "    input = input_types.ConversationInput(name=\"Conversation\", specification=input_types.EntityReferenceInput(id=specification_id),\n",
        "        filter=input_types.ContentCriteriaInput(collections=[input_types.EntityReferenceInput(id=collection_id)]))\n",
        "\n",
        "    conversation_id = None\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.create_conversation(input)\n",
        "\n",
        "        conversation_id = response.create_conversation.id if response.create_conversation is not None else None\n",
        "    except exceptions.GraphQLClientHttpError as e:\n",
        "        print(str(e))\n",
        "\n",
        "    if conversation_id is None:\n",
        "        print('Failed to create conversation.')\n",
        "        return None\n",
        "\n",
        "    print(f'Created conversation [{conversation_id}].')\n",
        "\n",
        "    try:\n",
        "        response = await graphlit.client.prompt_conversation(prompt, conversation_id)\n",
        "\n",
        "        message = response.prompt_conversation.message if response.prompt_conversation is not None else None\n",
        "\n",
        "        return message\n",
        "    except exceptions.GraphQLClientHttpError as e:\n",
        "        print(str(e))\n",
        "    finally:\n",
        "        _ = await graphlit.client.delete_conversation(conversation_id)"
      ],
      "metadata": {
        "id": "amGXokEKMcaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize Graphlit test"
      ],
      "metadata": {
        "id": "P36K0fK4NeEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: this will delete all contents and collections in project\n",
        "await delete_all_contents()\n",
        "await delete_all_collections()\n",
        "\n",
        "# Initialize specification\n",
        "specification_id = await create_specification()\n",
        "\n",
        "if specification_id is not None:\n",
        "    print(f'Created specification [{specification_id}].')\n",
        "else:\n",
        "    print('Failed to create specification.')"
      ],
      "metadata": {
        "id": "ZoJfJDHkT8At"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collection_id = None\n",
        "\n",
        "# Upload all essays\n",
        "try:\n",
        "    collection_id = await upload_graphlit_essays()\n",
        "\n",
        "    print(f'Essays ingested into collection [{collection_id}].')\n",
        "except exceptions.GraphQLClientHttpError as e:\n",
        "    print(str(e))"
      ],
      "metadata": {
        "id": "01bHgeL30Fux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validate Graphlit response"
      ],
      "metadata": {
        "id": "kwXchpK0Nw7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "benchmark_item = BenchmarkItem(\n",
        "    question=\"In what month and year was the talk regarding Lisp for web-based applications given?\",\n",
        "    answer=\"\"\n",
        ")\n",
        "\n",
        "if collection_id is not None and specification_id is not None:\n",
        "    message = await get_graphlit_rag_response(benchmark_item, specification_id, collection_id)\n",
        "\n",
        "    if message is not None:\n",
        "        print(message.message)\n",
        "        print(f'Tokens: {message.tokens}, took [{message.completion_time}]')"
      ],
      "metadata": {
        "id": "6xn58jvx5M2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform Graphlit test and score run"
      ],
      "metadata": {
        "id": "srzhQt4COLVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "\n",
        "async def run_test(specification_id, collection_id):\n",
        "    tasks = []\n",
        "    for item in benchmark.items:\n",
        "        task = get_graphlit_rag_response(item, specification_id, collection_id)\n",
        "        tasks.append(task)\n",
        "\n",
        "    return await asyncio.gather(*tasks)\n",
        "\n",
        "messages = await run_test(specification_id, collection_id)\n",
        "\n",
        "raw_graphlit_responses = []\n",
        "\n",
        "for message in messages:\n",
        "    raw_graphlit_responses.append(message.message.strip() if message is not None else None)"
      ],
      "metadata": {
        "id": "fOb6COcONZIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graphlit_responses = [\n",
        "    LLMResponse(\n",
        "        llm_answer=r, llm_context_list=[], benchmark_item=bi\n",
        "    ) for r, bi in zip(raw_graphlit_responses, benchmark.items)\n",
        "]"
      ],
      "metadata": {
        "id": "oX8zCf_4iM6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scorer = ValidateScorer(model_evaluator=\"gpt-4-turbo\", metrics=[AnswerSimilarityMetric()])\n",
        "graphlit_run = scorer.score_run(graphlit_responses, parallelism=5)"
      ],
      "metadata": {
        "id": "9b-528OPiR3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graphlit_run_df = run_to_dataframe(graphlit_run)\n",
        "graphlit_run_df.to_csv(f\"{tonic_validate_directory}/graphlit_run.csv\", index=False)"
      ],
      "metadata": {
        "id": "ba9aFP33iWz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize Graphlit test"
      ],
      "metadata": {
        "id": "2NOIhjyhOaeM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graphlit_answer_similarity_scores = pd.Series([x.scores[\"answer_similarity\"] for x in graphlit_run.run_data])\n",
        "category_counts = graphlit_answer_similarity_scores.value_counts()\n",
        "plt.bar(category_counts.index, category_counts.values)\n",
        "\n",
        "plt.title('Distribution of scores for graphlit')\n",
        "plt.xlabel('Score')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "plt.bar(category_counts.index, category_counts.values, color='#A679C8')\n",
        "\n",
        "# Remove all scores except whole numbers\n",
        "plt.xticks(range(0, 6, 1))\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "19lo0smiicf_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}